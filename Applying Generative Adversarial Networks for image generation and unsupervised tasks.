import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
import matplotlib.pyplot as plt
(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
# Normalize the data between -1 and 1 for GAN
x_train = (x_train - 127.5) / 127.5
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
print("Training data shape:", x_train.shape)
def build_generator():
model = tf.keras.Sequential()
model.add(layers.Dense(256, input_dim=100))
model.add(layers.LeakyReLU(alpha=0.2))
model.add(layers.BatchNormalization(momentum=0.8))
model.add(layers.Dense(512))
model.add(layers.LeakyReLU(alpha=0.2))
model.add(layers.BatchNormalization(momentum=0.8))
model.add(layers.Dense(1024))
model.add(layers.LeakyReLU(alpha=0.2))
model.add(layers.BatchNormalization(momentum=0.8))
model.add(layers.Dense(28 * 28 * 1, activation='tanh'))
model.add(layers.Reshape((28, 28, 1)))
return model
def build_discriminator():
model = tf.keras.Sequential()
model.add(layers.Flatten(input_shape=(28, 28, 1)))
model.add(layers.Dense(512))
model.add(layers.LeakyReLU(alpha=0.2))
model.add(layers.Dense(256))
model.add(layers.LeakyReLU(alpha=0.2))
model.add(layers.Dense(1, activation='sigmoid'))
return model
# Build and Compile Discriminator
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# Build Generator
generator = build_generator()
# GAN Model (Stacked)
z = layers.Input(shape=(100,))
img = generator(z)
discriminator.trainable = False
validity = discriminator(img)
# Compile GAN
gan = tf.keras.Model(z, validity)
gan.compile(loss='binary_crossentropy', optimizer='adam')
import os
epochs = 5000
batch_size = 128
save_interval = 1000
for epoch in range(epochs):
# Train Discriminator
idx = np.random.randint(0, x_train.shape[0], batch_size)
real_imgs = x_train[idx]
noise = np.random.normal(0, 1, (batch_size, 100))
gen_imgs = generator.predict(noise)
d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))
d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((batch_size, 1)))
d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
# Train Generator
noise = np.random.normal(0, 1, (batch_size, 100))
g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))
if epoch % save_interval == 0:
print(f"{epoch} [D loss: {d_loss[0]}] [G loss: {g_loss}]")
def plot_generated_images(generator, examples=25, dim=(5, 5)):
noise = np.random.normal(0, 1, (examples, 100))
generated_images = generator.predict(noise)
generated_images = 0.5 * generated_images + 0.5 # Rescale to [0,1]
plt.figure(figsize=(5, 5))
for i in range(generated_images.shape[0]):
plt.subplot(dim[0], dim[1], i+1)
plt.imshow(generated_images[i, :, :, 0], cmap='gray')
plt.axis('off')
plt.tight_layout()
plt.show()
plot_generated_images(generator)
