import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
import numpy as np
import os
# Generate synthetic binary classification dataset (e.g., using random noise as an example)
def create_synthetic_data(num_samples=1000, image_size=(64, 64)):
np.random.seed(42)
X = np.random.rand(num_samples, *image_size, 3)
y = np.random.randint(0, 2, num_samples) # Binary labels (0 or 1)
return X, y
# Step 1: Create dataset
image_size = (64, 64) # Image dimensions
num_samples = 1000
X, y = create_synthetic_data(num_samples, image_size)
# Step 2: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Step 3: Build the CNN model
model = models.Sequential([
layers.Conv2D(32, (3, 3), activation='relu', input_shape=(*image_size, 3)),
layers.MaxPooling2D((2, 2)),
layers.Conv2D(64, (3, 3), activation='relu'),
layers.MaxPooling2D((2, 2)),
layers.Conv2D(128, (3, 3), activation='relu'),
layers.MaxPooling2D((2, 2)),
layers.Flatten(),
layers.Dense(128, activation='relu'),
layers.Dense(1, activation='sigmoid') # Output layer for binary classification
])
# Step 4: Compile the model
model.compile(optimizer='adam',
loss='binary_crossentropy',
metrics=['accuracy'])
# Step 5: Train the model
history = model.fit(
X_train, y_train,
epochs=10,
batch_size=32,
validation_split=0.2
)
# Step 6: Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")
# Step 7: Save the model
model.save("binary_classification_cnn.h5")
print("Model saved successfully.")
